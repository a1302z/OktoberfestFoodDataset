{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pseudo Labeling\n",
    "<p>\n",
    "    Pseudo labeling is a semi-supervised learning technique. It means you train a model on little annotated data and apply it to a large set of not annotated data. The results you get from this are considered as new annotations, so you can train a new model with much more data, to the cost that there might be erroneous labels.\n",
    "    </p>\n",
    "    <p>\n",
    "    <b> This notebook is used as a playground to test things and implementations for the LargePseudoLabeling script </b>\n",
    "    </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from forward_pass import forward_pass, flatten_out_dict, seconds_to_str\n",
    "import numpy as np\n",
    "import sys\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from fnmatch import fnmatch\n",
    "import os\n",
    "import imageio\n",
    "import shutil\n",
    "from difflib import SequenceMatcher\n",
    "import time\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from object_detection.utils import dataset_util\n",
    "import shelve\n",
    "\n",
    "category_names = ['Bier', 'Bier Mass', 'Weissbier', 'Cola', 'Wasser', 'Curry-Wurst', 'Weisswein',\n",
    "                   'A-Schorle', 'Jaegermeister', 'Pommes', 'Burger', 'Williamsbirne', 'Alm-Breze', 'Brotzeitkorb',\n",
    "                   'Kaesespaetzle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def empty_directory(folder):\n",
    "    for the_file in os.listdir(folder):\n",
    "        file_path = os.path.join(folder, the_file)\n",
    "        try:\n",
    "            if os.path.isfile(file_path):\n",
    "                os.unlink(file_path)\n",
    "            #elif os.path.isdir(file_path): shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            \n",
    "def get_files(path, pattern, not_pattern = None, printout=True):\n",
    "    found = []\n",
    "    for path, subdirs, files in os.walk(path):\n",
    "        for name in files:\n",
    "            if fnmatch(name, pattern) and (not_pattern is None or not fnmatch(name, not_pattern)):\n",
    "                found.append(os.path.join(path, name))\n",
    "    if printout:\n",
    "        print(\"Found %d files in path %s\"%(len(found), path))\n",
    "    return found\n",
    "\n",
    "\"\"\"\n",
    "Source: https://gist.github.com/soply/f3eec2e79c165e39c9d540e916142ae1\n",
    "\"\"\"\n",
    "def show_images(images, cols = 1, titles = None):\n",
    "    \"\"\"Display a list of images in a single figure with matplotlib.\n",
    "    \n",
    "    Parameters\n",
    "    ---------\n",
    "    images: List of np.arrays compatible with plt.imshow.\n",
    "    \n",
    "    cols (Default = 1): Number of columns in figure (number of rows is \n",
    "                        set to np.ceil(n_images/float(cols))).\n",
    "    \n",
    "    titles: List of titles corresponding to each image. Must have\n",
    "            the same length as titles.\n",
    "    \"\"\"\n",
    "    assert((titles is None)or (len(images) == len(titles)))\n",
    "    n_images = len(images)\n",
    "    if titles is None: titles = ['Image (%d)' % i for i in range(1,n_images + 1)]\n",
    "    fig = plt.figure()\n",
    "    for n, (image, title) in enumerate(zip(images, titles)):\n",
    "        a = fig.add_subplot(cols, np.ceil(n_images/float(cols)), n + 1)\n",
    "        if image.ndim == 2:\n",
    "            plt.gray()\n",
    "        plt.imshow(image)\n",
    "        a.set_title(title)\n",
    "    fig.set_size_inches(np.array(fig.get_size_inches()) * n_images)\n",
    "    plt.show()\n",
    "    \n",
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find given video files and extract all frames as images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cams = [ 'Cam1']#, 'CamStereoL','Cam2', 'CamStereoR'] #StereoR ignored because not in sync\n",
    "days = range(18,19)#28)#'2018-05-26'\n",
    "videos_list = [[] for i in range(len(cams))]\n",
    "for d in days:\n",
    "    d_s = '2018-05-'+str(d)\n",
    "    for i, cam in enumerate(cams):\n",
    "        video_folder_id = cam+'/'+d_s\n",
    "        video_path = '/nfs/students/winter-term-2018/project_2/video_data/videos/'+ video_folder_id\n",
    "        pattern = \"*.mp4\"\n",
    "        videos_list[i].append(get_files(video_path, pattern, not_pattern='*._*', printout=False))# for video_path in video_paths]\n",
    "for i in range(len(cams)):\n",
    "    print(\"Found %d days with %d total files for cam %d\"%(len(videos_list[i]), sum([len(videos_list[i][j]) for j in range(len(videos_list[i]))]), i))\n",
    "#videos_list[0].sort()\n",
    "for i in range(len(videos_list)):\n",
    "    for j in range(len(videos_list[i])):\n",
    "        videos_list[i][j].sort() #[video.sort() for video in videos_list]\n",
    "#for i in range(4):\n",
    "#    for j in range(len(videos_list)):\n",
    "#        print(videos_list[j][i][-60:])\n",
    "#    print(\"----------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_data = [[] for c in cams]\n",
    "plot_stuff = True\n",
    "show_time_in_image = False\n",
    "print_every = 2\n",
    "picked_videos = range(0,10)#500,50) #[0, -1]\n",
    "all_images = False\n",
    "\n",
    "#data = shelve.open('tmp_data')\n",
    "data_count = 0\n",
    "\n",
    "\n",
    "id_str = '_'\n",
    "for i in picked_videos:\n",
    "    id_str += '%d_'%i\n",
    "\n",
    "plots = []\n",
    "titles = []\n",
    "print(\"Starting\")\n",
    "for day in range(len(days)):\n",
    "    if all_images:\n",
    "        picked_videos = range(min([len(videos_list[i][day]) for i in range(len(cams))]))\n",
    "    t_d = time.time()\n",
    "    for c, i in enumerate(picked_videos):\n",
    "        for j, cam in enumerate(videos_list):\n",
    "            t_v = time.time()\n",
    "            video = cam[day][i]\n",
    "            images_video = []\n",
    "            cap = cv2.VideoCapture(video)\n",
    "            success,image = cap.read()\n",
    "            count = 0\n",
    "            while success and count < 90:\n",
    "                #cv2.imwrite(\"frame%d.jpg\" % count, image)     # save frame as JPEG file  \n",
    "                #plt.imshow(image)\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                images_video.append(image)\n",
    "                success,image = cap.read()\n",
    "                count += 1\n",
    "            if count > 90:\n",
    "                print(\"WARNING: More than 90 images extracted\")\n",
    "            elif count < 90:\n",
    "                print(\"WARNING: Less than 90 images extracted\")\n",
    "            #print(\"Extracted %d images\"%len(images_video))\n",
    "            try:\n",
    "                img_data[j].append(np.array(images_video))\n",
    "            except Exception as e:\n",
    "                print(\"Exception: %s\"%str(e))\n",
    "                print(\"Data size: %f\"%sys.getsizeof(img_data[j]))\n",
    "            #data[str(data_count)] = np.array(images_video)\n",
    "            data_count += 1\n",
    "            if plot_stuff:\n",
    "                #plt.title(\"Video %d - Cam %d\"%(i, j))\n",
    "                titles.append('Video %d - Day %d - Cam %d'%(i,day,j))\n",
    "                if show_time_in_image:\n",
    "                    plots.append(images_video[int(len(images_video)/2)][:35,:340])\n",
    "                else:\n",
    "                    plots.append(images_video[int(len(images_video)/2)])    \n",
    "                #plt.show()\n",
    "            else:\n",
    "                if c % print_every == 0:\n",
    "                    t_v = time.time() - t_v\n",
    "                    print(\"%d videos loaded for cam %d (%.1fs per video)\"%(c,j, t_v))\n",
    "    t_d = time.time() - t_d\n",
    "    print(\"Completed loading videos of day %d/%d\\t Exp. time left: %s\"%(day+1, len(days), seconds_to_str(t_d*(len(days)-day-1))))\n",
    "if plot_stuff:\n",
    "    show_images(plots, cols=len(days)*len(picked_videos), titles = titles)\n",
    "img_data = [np.vstack(img_data[i]) for i in range(len(img_data))]\n",
    "#for i in range(len(img_data)):\n",
    "#    print(\"Img data shape for cam %i: %s\"%(i, str(img_data[i].shape)))\n",
    "#data['max_count'] = data_count\n",
    "#data.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify loaded data with model trained on manually annotated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'train/ssd_julius_mobilefpn_large/frozen_inference_graph.pb'\n",
    "out_dict = forward_pass(model, img_data[0], gpu='0', BATCH_SIZE=1, return_images=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out_dict.keys())\n",
    "print(len(out_dict['detection_classes']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter labels\n",
    "<p>\n",
    "    To suppress noise in annotations we only consider frames that are labeled consistently in a certain number of frames. Note that we do not consider frames without any annotations at all (empty frames).\n",
    "    </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh=0.5\n",
    "filter_value = 2\n",
    "previous_detection = None\n",
    "last_change = 0\n",
    "begin = 0\n",
    "already_printed = False\n",
    "filtered_image_assignments = [] #list of tuples with ([begin img, end img], dictionary)\n",
    "print_stuff = False\n",
    "for i in range(len(img_data[0])):\n",
    "    bxs = out_dict['detection_boxes'][i]\n",
    "    clss = out_dict['detection_classes'][i]\n",
    "    scr = out_dict['detection_scores'][i]\n",
    "    detection = None\n",
    "    for j in range(len(clss)):\n",
    "        if scr[j] < thresh:\n",
    "            continue\n",
    "        if detection is None:\n",
    "            detection = {i: 0 for i in range(len(category_names))}\n",
    "        detection[int(clss[j])] += 1\n",
    "    #if detection is not None:\n",
    "    if detection == previous_detection:\n",
    "        if detection is not None and i - last_change > filter_value and not already_printed:\n",
    "            out_str = \"Detected items for image %i\"%(i-filter_value)\n",
    "            for key in detection:\n",
    "                if detection[key] > 0:\n",
    "                    out_str += \"\\n  - %d %s\"%(detection[key], category_names[key])\n",
    "            if print_stuff:\n",
    "                print(out_str)\n",
    "            already_printed = True\n",
    "    else:\n",
    "        if previous_detection is not None and i - last_change - 1 > filter_value:\n",
    "            if print_stuff:\n",
    "                print(\"until image %d\\n\"%i)\n",
    "            filtered_image_assignments.append(([begin, i], previous_detection))\n",
    "        last_change = i\n",
    "        already_printed = False\n",
    "        begin = i+1\n",
    "    previous_detection = detection\n",
    "print(\"Found %d filtered assignments\"%len(filtered_image_assignments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in filtered_image_assignments[:10]:\n",
    "    show_images([i[int((x[0][1]+x[0][0])/2)] for i in img_data], titles=['Cam %d'%i for i in range(len(cams))])#[:35,:340]\n",
    "    print(\"%d frames\"%(x[0][1]-x[0][0]))\n",
    "    for key in x[1]:\n",
    "        if x[1][key] > 0:\n",
    "            print(' - %d x %s'%(x[1][key], category_names[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img_data[0][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new dataset with pseudo labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def create_tf_example(index, empty=False, thresh=0.5):\n",
    "    # TODO(user): Populate the following variables from your example.\n",
    "    example = img_data[0][index]\n",
    "    height = example.shape[0] # Image height\n",
    "    width = example.shape[1] # Image width\n",
    "    filename = str.encode('') # Filename of the image. Empty if image is not from file\n",
    "    encoded_image_data = cv2.imencode('.png', example)\n",
    "    encoded_image_data = encoded_image_data[1].tostring() # Encoded image bytes\n",
    "    image_format = str.encode('png') # b'jpeg' or b'png'\n",
    "\n",
    "    xmins = [] # List of normalized left x coordinates in bounding box (1 per box)\n",
    "    xmaxs = [] # List of normalized right x coordinates in bounding box\n",
    "             # (1 per box)\n",
    "    ymins = [] # List of normalized top y coordinates in bounding box (1 per box)\n",
    "    ymaxs = [] # List of normalized bottom y coordinates in bounding box\n",
    "             # (1 per box)\n",
    "    classes_text = [] # List of string class name of bounding box (1 per box)\n",
    "    classes = [] # List of integer class id of bounding box (1 per box)\n",
    "    \n",
    "    if not empty:\n",
    "        bxs = out_dict['detection_boxes'][i]\n",
    "        clss = out_dict['detection_classes'][i]\n",
    "        scr = out_dict['detection_scores'][i]\n",
    "        #clss = [0, 1, 2]\n",
    "        #scr = [0.9, 0.9, 0.9]\n",
    "        #print(max(scr))\n",
    "        #print(\"Len classes: %d\"%sum(i > thresh for i in scr))\n",
    "        for j in range(len(clss)):\n",
    "            if scr[j] < thresh:\n",
    "                continue\n",
    "            classes.append(clss[j])\n",
    "            classes_text.append(str.encode(category_names[clss[j]]))\n",
    "            ymins.append(bxs[j][0])\n",
    "            xmins.append(bxs[j][1])\n",
    "            ymaxs.append(bxs[j][2])\n",
    "            xmaxs.append(bxs[j][3])\n",
    "\n",
    "    tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
    "      'image/height': dataset_util.int64_feature(height),\n",
    "      'image/width': dataset_util.int64_feature(width),\n",
    "      'image/filename': dataset_util.bytes_feature(filename),\n",
    "      'image/source_id': dataset_util.bytes_feature(filename),\n",
    "      'image/encoded': dataset_util.bytes_feature(encoded_image_data),\n",
    "      'image/format': dataset_util.bytes_feature(image_format),\n",
    "      'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
    "      'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
    "      'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
    "      'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
    "      'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
    "      'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
    "    }))\n",
    "    return tf_example\n",
    "\n",
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()    \n",
    "    keys_list = [keys for keys in flags_dict]    \n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(_bytes_feature(img_data[0][0].tobytes()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include_empty = False\n",
    "\n",
    "add_name = '_notebook_test'\n",
    "del_all_flags(tf.flags.FLAGS)\n",
    "flags = tf.app.flags\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "flags.DEFINE_string('output_path_train', 'data/test'+add_name, 'Path to output TFRecord train')\n",
    "flags.DEFINE_string('output_path_eval', 'data/test'+add_name, 'Path to output TFRecord eval')\n",
    "flags.DEFINE_string('output_path', 'data/test'+add_name, 'Path to output TFRecord')\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "print(FLAGS.output_path_train)\n",
    "\n",
    "out_file = FLAGS.output_path\n",
    "if os.path.isfile(out_file):\n",
    "    os.remove(out_file)\n",
    "writer = tf.python_io.TFRecordWriter(FLAGS.output_path)\n",
    "    \n",
    "last = 0\n",
    "#\"\"\"\n",
    "for filtered_label in filtered_image_assignments:\n",
    "    if include_empty:\n",
    "        for i in range(last, filtered_label[0][0]):\n",
    "            tf_example = create_tf_example(i, empty=True)\n",
    "            writer.write(tf_example.SerializeToString())\n",
    "    for i in range(filtered_label[0][0], filtered_label[0][1]):\n",
    "        tf_example = create_tf_example(i)\n",
    "        writer.write(tf_example.SerializeToString())\n",
    "#\"\"\"\n",
    "#for i in range(10):\n",
    "#    ex = create_tf_example(i, empty = False, thresh=0.5)\n",
    "#    writer.write(ex.SerializeToString())\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As a sanity check we read data again, display it and compare to manually labeled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'sess' in locals() and sess is not None:\n",
    "    sess.close()\n",
    "new_data = '/nfs/students/winter-term-2018/project_2/models/research/object_detection/training_folder_lsml/data/pseudo_labels_days-23-28'\n",
    "old_data = '/nfs/students/winter-term-2018/project_2/models/research/object_detection/training_folder_lsml/data/pseudo_labels_days-18-old_encoding'\n",
    "verified_data = '/nfs/students/winter-term-2018/project_2/models/research/object_detection/training_folder_lsml/data/train_data_beer_sn_reviewed'\n",
    "test_data = '/nfs/students/winter-term-2018/project_2/models/research/object_detection/training_folder_lsml/data/test_notebook_test'\n",
    "#print(img_data[0][0].shape)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = ''\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_iterator = tf.python_io.tf_record_iterator(path=verified_data)\n",
    "\n",
    "num = 5000\n",
    "every_n = 10\n",
    "images = []\n",
    "titles = []\n",
    "count = 0\n",
    "print(\"Start\")\n",
    "time_start = time.time()\n",
    "stats = {c: 0 for c in category_names}\n",
    "for string_record in record_iterator:\n",
    "    #if count % every_n != 0:\n",
    "    #    count += 1\n",
    "    #    if count >= num:\n",
    "    #        break\n",
    "    #    continue\n",
    "    example = tf.train.Example()\n",
    "    example.ParseFromString(string_record)\n",
    "    d = dict(example.features.feature)\n",
    "    #for key in d:\n",
    "    #    if 'encoded' not in key:\n",
    "    #        print(\"Key: %s\"%key)\n",
    "    #        print(d[key])\n",
    "    #print(d.keys())\n",
    "    #print(\"bbox_min: %s\"%d['image/object/bbox/xmin'])\n",
    "    #print(\"class label%s\"%d['image/object/class/label'])\n",
    "    #print(\"img height: %s\"%d['image/height'].int64_list.value[0])\n",
    "    text = [x.decode() for x in d['image/object/class/text'].bytes_list.value]\n",
    "    for t in text:\n",
    "        stats[t] += 1\n",
    "        if t not in stats:\n",
    "            print(t)\n",
    "    if count % every_n == 0:\n",
    "        try:\n",
    "            img = tf.image.decode_jpeg(d['image/encoded'].bytes_list.value[0])\n",
    "            img = img.eval()\n",
    "            #print(\"Image shape %s\"%str(img.shape))\n",
    "            #plt.imshow(img)\n",
    "            #img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            images.append(img)\n",
    "            print('.', end='')\n",
    "            titles.append(str(text))\n",
    "        except Exception as e:\n",
    "            print(\"EXCEPTION\")\n",
    "            print(e)\n",
    "    \n",
    "    # Exit after 1 iteration as this is purely demonstrative.\n",
    "    count += 1\n",
    "    if count >= num:\n",
    "        break\n",
    "    #print('\\n--------------\\n')\n",
    "print(\"\\nDone\")\n",
    "print(stats)\n",
    "print(\"%d datapoints took %s\"%(count, seconds_to_str(time.time()-time_start)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total of %d items in record (%d annotations)\"%(count, sum(stats.values())))\n",
    "for key in stats:\n",
    "    print('%d \\tx %s'%(stats[key], key))\n",
    "plt.bar(stats.keys(), stats.values())\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, m = 0, count\n",
    "show_images(images[n:m], titles=titles[n:m], cols = len(images[n:m]))# max(1, count/(1*every_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-tf",
   "language": "python",
   "name": "py-tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
